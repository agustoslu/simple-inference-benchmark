{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.utils import enable_info_logs\n",
    "\n",
    "enable_info_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import load_human_labels\n",
    "\n",
    "\n",
    "human_labels, questions, comment_cols = load_human_labels()\n",
    "human_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_balance = (\n",
    "    human_labels[questions].apply(lambda s: s.value_counts(normalize=True)).round(3)\n",
    ")\n",
    "label_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_scalars_for_questions\n",
    "\n",
    "plot_scalars_for_questions(\n",
    "    label_balance.max(axis=0), questions, \"Majority Class %\", x_reversed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage Agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels_long = pd.melt(\n",
    "    human_labels, id_vars=[\"classification_by\", \"post_id\"], value_vars=questions\n",
    ")\n",
    "human_labels_long.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import compute_agreement_score\n",
    "\n",
    "human_agreement = compute_agreement_score(human_labels_long)\n",
    "human_agreement.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_by_question = human_agreement.groupby(\"variable\", as_index=False).agg(\n",
    "    avg_agreement_score=(\"agreement_score\", \"mean\"),\n",
    "    avg_full_agreement=(\"full_agreement\", \"mean\"),\n",
    ")\n",
    "agreement_by_question.sort_values(\"avg_full_agreement\", inplace=True)\n",
    "agreement_by_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scalars_for_questions(\n",
    "    agreement_by_question[\"avg_full_agreement\"],\n",
    "    agreement_by_question[\"variable\"],\n",
    "    \"Posts with Full Agreement [%]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bench_lib.evaluation import krippendorf_alpha\n",
    "\n",
    "\n",
    "alphas = [\n",
    "    krippendorf_alpha(human_labels[\"post_id\"], human_labels[q]) for q in questions\n",
    "]\n",
    "fig = plot_scalars_for_questions(\n",
    "    alphas, questions, \"Krippendorff's alpha\", x_reversed=True\n",
    ")\n",
    "# fig.savefig(\"imgs/krippendorffs_alpha.pdf\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AI perform on the easy posts?\n",
    "Assuming the easy posts are those where humans are consistent.\n",
    "We assume here the ground truth to be the human labels, which are unique by construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_long = pd.merge(\n",
    "    human_labels_long,\n",
    "    human_agreement.query(\"full_agreement\"),\n",
    "    on=(\"post_id\", \"variable\"),\n",
    ").drop_duplicates()\n",
    "ground_truth_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import load_ai_labels, compute_ai_perfs\n",
    "\n",
    "\n",
    "gemma3_folders = [\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-27b-it_00\",\n",
    "    \"gemini-2.5-pro-noschema\",\n",
    "]\n",
    "gemma3_ai_labels_long = load_ai_labels(gemma3_folders, questions, comment_cols)\n",
    "gemma3_ai_perfs = compute_ai_perfs(ground_truth_long, gemma3_ai_labels_long, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sizes = ground_truth_long.groupby(\"variable\", as_index=False).size()\n",
    "plot_scalars_for_questions(gt_sizes[\"size\"], gt_sizes[\"variable\"], \"Ground truth size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_ai_perfs\n",
    "\n",
    "\n",
    "g3_order = [f\"google/gemma-3-{n}b-it\" for n in [4, 12, 27]] + [\n",
    "    \"google/gemini-2.5-pro-preview-03-25\"\n",
    "]\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "for y in metrics:\n",
    "    fig = plot_ai_perfs(gemma3_ai_perfs, g3_order, list(reversed(questions)), y=y)\n",
    "    fig.savefig(f\"imgs/gemma3_ai_perfs_{y}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_ai_labels = load_ai_labels(\n",
    "    folders=[\"qwen-2.5-vl\", \"gemini-2.5-pro-noschema\"],\n",
    "    questions=questions,\n",
    "    comment_cols=comment_cols,\n",
    ")\n",
    "qwen_ai_labels.query(\n",
    "    \"`Model ID`.str.contains('Qwen') or `Model ID`.str.contains('gemini')\", inplace=True\n",
    ")\n",
    "qwen_ai_perfs = compute_ai_perfs(ground_truth_long, qwen_ai_labels, questions)\n",
    "order = [f\"Qwen/Qwen2.5-VL-{n}B-Instruct\" for n in [3, 7, 72]] + [\n",
    "    \"google/gemini-2.5-pro-preview-03-25\"\n",
    "]\n",
    "for y in metrics:\n",
    "    fig = plot_ai_perfs(qwen_ai_perfs, order, x_order=list(reversed(questions)), y=y)\n",
    "    fig.savefig(f\"imgs/qwen_ai_perfs_{y}.pdf\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
