{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74fec33b-6445-4d30-92b1-b503ceabca46",
   "metadata": {
    "id": "74fec33b-6445-4d30-92b1-b503ceabca46"
   },
   "source": [
    "## Exercise for Lecture 8: Evaluation Protocol II -- Automatic\n",
    "### Profilierungsmodul CL I, MSc Computational Linguistics: Trustworthy Data-Centric AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82661fb-0b1f-4df2-87a7-9cc61c20528e",
   "metadata": {},
   "source": [
    "__Context__: \\\n",
    "You are conducting an evaluation among human-written and model-generated summaries. \\\n",
    "You received these following 10 summaries, 5 written by human __human_summaries__ and five generated by language models __model_summaries__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcd580-b076-452a-830d-dad82b28b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_summaries = [\n",
    "\"Gemma, 23, has left her children to be raised by her mother. Handed them to her 52-year-old parent when they were four months old. Spends her time partying and has 'missed out' on seeing them grow up. Is now at risk of being banned from seeing them thanks to feckless ways. Mother Debbie also says the unemployed 23-year-old has stolen from them.\",\n",
    "\"23-year-old mother Gemma may be forbidden to see her children by her mother Debbie, who cares for the children. Debbie says this is because of Gemma's stealing and selling property belonging to the children, and prioritizing drinking, drugs and partying over the children.\",\n",
    "\"A mother of two is about to be denied access to her children. The woman is an alcoholic and a drug addict who refuses to seek help.  She has stolen from her mother and her children to afford her nasty habits.\",\n",
    "\"Gemma is at risk of losing her children because she does not want to stop drinking and partying. Gemma's mother Debbie is caring for her children. Gemma needs to grow up and leave the drugs, parties and drinking behind.\",\n",
    "\"Gemma is going to lose access to her children because her mother cannot trust her to stay sober and stop stealing from her. Gemma's mother, Debbie, is taking care of the kids who are her grandchildren. According to Debbie, Gemma needs to get sober and act mature in order to see her kids again.\"]\n",
    "model_summaries = [\n",
    "\"the woman , named only as gemma , has two children under five by two different fathers and handed both infants over to her 52-year-old mother debbie when they were four months old . a 23-year-old mother-of-two is at risk of being banned from seeing her own children - because she refuses to stop drinking and partying . now debbie is threatening to ban gemma from seeing the children at all , after discovering her daughter 's penchant for legal high , mkat , and because she suspects her of stealing .\",\n",
    "\"Gemma, a mother-of-two, is at risk of being banned from seeing her own children because she refuses to stop drinking and partying 'I don't want to be responsible for her having children,' says Debbie. 'I don't want to be involved with her. I'm not going to let her do that to my children.\", \n",
    "\"a 23-year-old mother-of-two is at risk of being banned from seeing her own children - because she refuses to stop drinking and partying . the woman , named only as gemma , has two children under five by two different fathers and handed both infants over to her 52-year-old mother debbie when they were four months old . now debbie is threatening to ban gemma from seeing the children at all , after discovering her daughter 's penchant for legal high , mkat , and because she suspects her of stealing .\",\n",
    "\"Gemma , 23 , has two children under five by two different fathers . Handed both infants over to her 52-year-old mother Debbie when they were four months old . Now Debbie is threatening to ban Gemma from seeing the children at all . Says her daughter 's constant drinking and partying is getting out of hand .\",\n",
    "\"the woman has two children under five by two different fathers . debbie is threatening to ban gemma from seeing children at all . she says her daughter 's constant drinking is getting out of hand . she also claims gemma , who is unemployed , stole an ipad from one of the children .\"\n",
    "]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937d831-89a8-40d7-a14b-96225881cfb7",
   "metadata": {
    "id": "b937d831-89a8-40d7-a14b-96225881cfb7"
   },
   "source": [
    "### Exercise 1: Inter-Annotator Agreement\n",
    "Your first task is to assess whether humans successfully differentiate human written summaries. \\\n",
    "So you shuffled these 10 summaries in a random order and asked three annotators John, Mary, and Bill to annotate whether each of the 10 summaries is human-written (\"H\") or model-generated (\"M\"). \\\n",
    "Here are the gold labels and the responses from three human annotators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d945b2-567a-4f00-ab83-23c4a2a96efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = [\"H\", \"H\", \"M\", \"M\", \"H\", \"M\", \"H\", \"M\", \"M\", \"H\"]\n",
    "john = [\"H\", \"H\", \"M\", \"M\", \"H\", \"H\", \"H\", \"M\", \"M\", \"H\"]\n",
    "mary = [\"H\", \"H\", \"M\", \"M\", \"H\", \"M\", \"M\", \"M\", \"H\", \"H\"]\n",
    "bill = [\"M\", \"H\", \"H\", \"M\", \"H\", \"M\", \"H\", \"M\", \"M\", \"H\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5cf66-2139-41d2-8519-5f71a64e4c49",
   "metadata": {},
   "source": [
    "#### Exercise 1.1: Accuracy, Precision, Recall, F1\n",
    "Can you implement your code in Python and compare John, Mary and Bill's annotations to gold in terms of accuracy, precision, recall, and macro f1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5381fa3-02d1-49d6-bf2c-4aaf8fca9bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scikit-learn, pandas \n",
    "!pip install scikit-learn \n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28cb38-0fae-49b1-b5e3-29ffcfeda6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "def acc_p_r_f1(gold, predictions):\n",
    "    accuracy = accuracy_score(gold, predictions)\n",
    "    precision = precision_score(gold, predictions, average='macro')\n",
    "    recall = recall_score(gold, predictions, average='macro')\n",
    "    f1 = f1_score(gold, predictions, average='macro')\n",
    "    return accuracy, float(precision), float(recall), float(f1)\n",
    "\n",
    "# Evaluation\n",
    "john_metrics = acc_p_r_f1(gold, john)\n",
    "mary_metrics = acc_p_r_f1(gold, mary)\n",
    "bill_metrics = acc_p_r_f1(gold, bill)\n",
    "\n",
    "print(john_metrics)\n",
    "print(mary_metrics)\n",
    "print(bill_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd14510a-ff50-4da0-a3dc-1487ec69c468",
   "metadata": {},
   "source": [
    "#### Exercise 1.2: Pair-wise Raw & Kappa agreement \n",
    "Can you compute averaged pair-wise raw and kappa agreement among the three annotators? \\\n",
    "How much does raw agreement differ from cohen's kappa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f4b5d-aa41-45a1-8360-36af2d59bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "from statistics import mean\n",
    "def raw(A,B):\n",
    "    return 1.0*sum([A[i]==B[i] for i in range(len(A))])/len(A)\n",
    "kappas = []\n",
    "raws = []\n",
    "annotations = [john, mary, bill]\n",
    "for i in range(len(annotations)):\n",
    "    for j in range(i+1, len(annotations)):\n",
    "        kappas.append(float(cohen_kappa_score(annotations[i], annotations[j])))\n",
    "        raws.append(raw(annotations[i], annotations[j]))\n",
    "print(kappas)\n",
    "print(raws)\n",
    "print(mean(kappas))\n",
    "print(mean(raws))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f44355-6ff3-4f34-bd77-ce61b33477b9",
   "metadata": {},
   "source": [
    "#### Exercise 1.3: Krippendorff Alpha\n",
    "Since Krippendorff Alpha incorporates multiple annotators, can you also calculate it among the three annotators? \\\n",
    "How is the Krippendorff Alpha agreement compared to Cohen's Kappa?\n",
    "- Note that the python package for Krippendorff Alpha arranges and compares annotations by instances (rather than by annotators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c50a365-5b3e-4b99-a329-7a72d58d5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install krippendorff \n",
    "!pip install krippendorff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d340a-3c7f-40d5-a289-9440cffca376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from krippendorff import alpha\n",
    "\n",
    "# Prepare data for Krippendorff's alpha\n",
    "# Each row represents an item, and each column represents an annotator\n",
    "instances = pd.DataFrame(annotations).transpose().values.tolist()\n",
    "print(instances)\n",
    "\n",
    "# Calculate Krippendorff's alpha\n",
    "krippendorff_alpha = alpha(reliability_data=instances, level_of_measurement=\"nominal\")\n",
    "\n",
    "print(\"Krippendorff's Alpha:\", krippendorff_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e34fa-0636-40fb-a4cf-c90592f19381",
   "metadata": {
    "id": "864e34fa-0636-40fb-a4cf-c90592f19381"
   },
   "source": [
    "### Exercise 2: NLG Evaluation\n",
    "Now we are interested in looking at how the model summaries are different from human summaries. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e320f674-143a-4212-8a37-37570e9b4ec8",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Tokenize your summaries \n",
    "For pre-processing, you would need to tokenize your summaries before feeding to BLEU or ROUGE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974bdc3-6b24-47ca-a767-bdcd4b582ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nltk\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8849f7-6a52-4d0e-acfd-e44273d511d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_human_summaries = [word_tokenize(summary.lower()) for summary in human_summaries]\n",
    "tokenized_model_summaries = [word_tokenize(summary.lower()) for summary in model_summaries]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2773868-8f02-4737-9bfa-a1e76ac7b993",
   "metadata": {
    "id": "b2773868-8f02-4737-9bfa-a1e76ac7b993"
   },
   "source": [
    "#### Exercise 2.2: BLEU\n",
    "Compare the five model summaries against all human summaries using precision-based BLEU score. \\\n",
    "Which model summary scores the best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae413ae-00d9-4afc-bb1e-f0ea937db256",
   "metadata": {
    "id": "0ae413ae-00d9-4afc-bb1e-f0ea937db256"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# Compute BLEU score with smoothing\n",
    "smooth_fn = SmoothingFunction().method1  # Smoothing method for better BLEU scores on short sequences\n",
    "\n",
    "bleu_matrix = np.zeros((len(tokenized_human_summaries), len(tokenized_model_summaries)))\n",
    "\n",
    "for i in range(len(tokenized_human_summaries)):\n",
    "    for j in range(len(tokenized_model_summaries)):\n",
    "        score = sentence_bleu([tokenized_human_summaries[i]], tokenized_model_summaries[j], smoothing_function=smooth_fn)\n",
    "        bleu_matrix[i, j] = score\n",
    "        # print(f\"human {i} vs model {j}: BLEU Score = {score:.4f}\")\n",
    "\n",
    "bleu_df = pd.DataFrame(\n",
    "    bleu_matrix,\n",
    "    index=[f\"Human {i+1}\" for i in range(len(tokenized_human_summaries))],\n",
    "    columns=[f\"Model {j+1}\" for j in range(len(tokenized_model_summaries))]\n",
    ")\n",
    "\n",
    "print(bleu_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf5f40-93d0-4ba1-a8d8-304c1d9bf135",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: ROUGE\n",
    "Unlike BLEU, ROUGE is recall based. \\\n",
    "Compare the five model summaries against all human summaries using ROUGE-1, ROUGE-2, ROUGE-L. \\\n",
    "Which model summary scores the best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884da130-da3a-45e8-8566-e00c812d3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rouge-score\n",
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3cbbf-9739-492e-9fb8-73ab2ca7ff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "r_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_1 = np.zeros((len(human_summaries), len(model_summaries)))\n",
    "rouge_2 = np.zeros((len(human_summaries), len(model_summaries)))\n",
    "rouge_l = np.zeros((len(human_summaries), len(model_summaries)))\n",
    "\n",
    "for i in range(len(human_summaries)):\n",
    "    for j in range(len(model_summaries)):\n",
    "        r_score = r_scorer.score(human_summaries[i], model_summaries[j])\n",
    "        # print(r_score)\n",
    "        rouge_1[i, j] = r_score[\"rouge1\"].fmeasure\n",
    "        rouge_2[i, j] = r_score[\"rouge2\"].fmeasure\n",
    "        rouge_l[i, j] = r_score[\"rougeL\"].fmeasure\n",
    "        \n",
    "rouge_1_df = pd.DataFrame(\n",
    "    rouge_1,\n",
    "    index=[f\"Human {i+1}\" for i in range(len(human_summaries))],\n",
    "    columns=[f\"Model {j+1}\" for j in range(len(model_summaries))]\n",
    ")\n",
    "print(rouge_1_df)\n",
    "\n",
    "rouge_2_df = pd.DataFrame(\n",
    "    rouge_2,\n",
    "    index=[f\"Human {i+1}\" for i in range(len(human_summaries))],\n",
    "    columns=[f\"Model {j+1}\" for j in range(len(model_summaries))]\n",
    ")\n",
    "print(rouge_2_df)\n",
    "\n",
    "rouge_l_df = pd.DataFrame(\n",
    "    rouge_l,\n",
    "    index=[f\"Human {i+1}\" for i in range(len(human_summaries))],\n",
    "    columns=[f\"Model {j+1}\" for j in range(len(model_summaries))]\n",
    ")\n",
    "print(rouge_l_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519a340-7706-4714-97e3-6df10f462582",
   "metadata": {},
   "source": [
    "#### Exercise 2.4 BERTScore\n",
    "Time for some embedding-based scorer. \\\n",
    "The BERT scorer gives precision, recall, F1 scores based on cosine similarity between contextual embeddings. \\\n",
    "How do the BERTScores compare to BLEU and ROUGE? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79512866-cead-4f85-a96d-a8586ecca8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install rouge-score\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967e940-0696-4a29-b2da-90352bdcb98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score as b_score\n",
    "bert_matrix = np.zeros((len(tokenized_human_summaries), len(tokenized_model_summaries)))\n",
    "\n",
    "for i in range(len(human_summaries)):\n",
    "    for j in range(len(model_summaries)):\n",
    "        P, R, F1 = b_score([human_summaries[i]], [model_summaries[j]], lang=\"en\", verbose=False)\n",
    "        bert_matrix[i, j] = F1\n",
    "\n",
    "bert_df = pd.DataFrame(\n",
    "    bert_matrix,\n",
    "    index=[f\"Human {i+1}\" for i in range(len(human_summaries))],\n",
    "    columns=[f\"Model {j+1}\" for j in range(len(model_summaries))]\n",
    ")\n",
    "\n",
    "print(bert_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8c221-c98a-4558-91aa-0f8a997a6dc3",
   "metadata": {},
   "source": [
    "#### Exercise 2.5 Multi-reference evaluation\n",
    "Metrics like BLEU allow comparing one prediction to multiple references. \\\n",
    "Can you now compare each of the five model summaries to all five human summaries using BLEU. \n",
    "What do you observe? \\\n",
    "Do you have different observations from pair-wise comparisons? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c891ff-01bb-48cf-b377-67b8c7be6cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(tokenized_model_summaries)):\n",
    "    score = sentence_bleu(tokenized_human_summaries, tokenized_model_summaries[j], smoothing_function=smooth_fn)\n",
    "    print(f\"Model {j+1}\", score)\n",
    "\n",
    "print(\"\\nSingle-reference BLEU for comparison:\")\n",
    "print(bleu_df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
