{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "fpath = (\n",
    "    Path(os.environ[\"DSS_HOME\"])\n",
    "    / \"toxicainment/2025-02-07-saxony-labeled-data/human-labels.csv\"\n",
    ")\n",
    "human_labels = pd.read_csv(fpath, dtype={\"post_id\": str})\n",
    "human_labels = (\n",
    "    human_labels.sort_values(by=\"timestamp\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"is_saxony_election\": \"is_saxony\",\n",
    "            \"is_saxony_election_comment\": \"is_saxony_comment\",\n",
    "        }\n",
    "    )  # The column name was wrong while saving, but the UI correctly displayed that it does not HAVE to be politics in saxony\n",
    "    .groupby([\"post_id\", \"classification_by\"])\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")\n",
    "print(len(human_labels))\n",
    "# join num raters per post\n",
    "nraters_by_post = (\n",
    "    human_labels.groupby(\"post_id\", as_index=False)[\"classification_by\"]\n",
    "    .agg([\"nunique\"])\n",
    "    .rename(columns={\"nunique\": \"nraters\"})\n",
    ")\n",
    "human_labels = pd.merge(human_labels, nraters_by_post, on=\"post_id\", how=\"left\")\n",
    "human_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"is_political\",\n",
    "    \"is_saxony\",\n",
    "    \"is_intolerant\",\n",
    "    \"is_hedonic_entertainment\",\n",
    "    \"is_eudaimonic_entertainment\",\n",
    "]\n",
    "comment_cols = [f\"{col}_comment\" for col in questions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_balance = (\n",
    "    human_labels[questions].apply(lambda s: s.value_counts(normalize=True)).round(3)\n",
    ")\n",
    "label_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_scalars_for_questions\n",
    "\n",
    "plot_scalars_for_questions(label_balance.max(axis=0), questions, \"Majority Class %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage Agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement(human_labels: pd.DataFrame, question: str) -> float:\n",
    "    df = pd.crosstab(human_labels[\"post_id\"], human_labels[question])\n",
    "    full_agreement = (df[\"no\"] == 0) | (df[\"yes\"] == 0)\n",
    "    return float(full_agreement.mean())\n",
    "\n",
    "\n",
    "agreements = [compute_agreement(human_labels, q) for q in questions]\n",
    "print(agreements)\n",
    "plot_scalars_for_questions(agreements, questions, \"Percentage Agreement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bench_lib.evaluation import krippendorf_alpha\n",
    "\n",
    "\n",
    "alphas = [\n",
    "    krippendorf_alpha(human_labels[\"post_id\"], human_labels[q]) for q in questions\n",
    "]\n",
    "plot_scalars_for_questions(alphas, questions, \"Krippendorff's alpha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[round(a, 2) for a in alphas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fleis Kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import fleiss_kappa\n",
    "\n",
    "n_raters = 3\n",
    "q = \"is_intolerant\"\n",
    "fk_human_labels = human_labels.query(\"nraters == @n_raters\")\n",
    "table = pd.crosstab(fk_human_labels[\"post_id\"], fk_human_labels[q]).values\n",
    "kp, agreements = fleiss_kappa(table, method=\"fleiss\")\n",
    "ag_counts = pd.Series(agreements).value_counts()\n",
    "print(kp)\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "plt.bar(ag_counts.index, ag_counts.values, width=0.05)\n",
    "plt.xlim(-0.1, 1.1)\n",
    "plt.xlabel(\"Agreement\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Agreement for '{q}' with {n_raters} raters. Fleiss Kappa: {kp:.2f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from bench_lib.evaluation import difficulty_score, get_means\n",
    "\n",
    "q_mens = {q: get_means(human_labels, question=q) for q in questions}\n",
    "difficulty = difficulty_score(pd.DataFrame(q_mens))\n",
    "\n",
    "diff_counts = difficulty.value_counts()\n",
    "fig = plt.figure()\n",
    "plt.bar(diff_counts.index, diff_counts.values, width=0.03)\n",
    "plt.xlabel(\"Difficulty\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Post Difficulty\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_difficulty_granular = pd.melt(\n",
    "    (human_labels.groupby(\"post_id\")[questions].nunique() == 1).reset_index(),\n",
    "    id_vars=\"post_id\",\n",
    "    value_vars=questions,\n",
    "    value_name=\"human_consistent\",\n",
    ")\n",
    "post_difficulty_granular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_answers_by_post = (\n",
    "    human_labels.groupby(\"post_id\")[questions].nunique().max(axis=1)\n",
    ")\n",
    "hard_posts = unique_answers_by_post[unique_answers_by_post > 1]\n",
    "easy_posts = unique_answers_by_post[unique_answers_by_post == 1]\n",
    "print(len(hard_posts), len(easy_posts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_difficulty = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame({\"post_id\": hard_posts.index, \"difficulty\": \"hard\"}),\n",
    "        pd.DataFrame({\"post_id\": easy_posts.index, \"difficulty\": \"easy\"}),\n",
    "    ]\n",
    ")\n",
    "post_difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_difficulty[\"difficulty\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AI perform on the easy posts?\n",
    "We assume here the ground truth to be the human labels, which are unique by construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import load_ai_labels, compute_ai_perfs\n",
    "\n",
    "\n",
    "human_labels_easy_set = human_labels.query(\"post_id in @easy_posts.index\")[\n",
    "    [\"post_id\", *questions]\n",
    "].drop_duplicates()\n",
    "human_labels_easy_set\n",
    "\n",
    "gemma3_folders = [\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-27b-it_00\",\n",
    "    \"gemini-2.0-flash-001\",\n",
    "]\n",
    "gemma3_ai_labels = load_ai_labels(gemma3_folders, questions, comment_cols)\n",
    "gemma3_ai_perfs = compute_ai_perfs(human_labels_easy_set, gemma3_ai_labels, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_ai_perfs\n",
    "\n",
    "\n",
    "g3_order = [f\"google/gemma-3-{n}b-it\" for n in [4, 12, 27]] + [\n",
    "    \"google/gemini-2.0-flash-001\"\n",
    "]\n",
    "plot_ai_perfs(gemma3_ai_perfs, g3_order, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_ai_labels = load_ai_labels(\n",
    "    folders=[\"qwen-2.5-vl\", \"gemini-2.0-flash-001\"],\n",
    "    questions=questions,\n",
    "    comment_cols=comment_cols,\n",
    ")\n",
    "qwen_ai_labels.query(\n",
    "    \"`Model ID`.str.contains('Qwen') or `Model ID`.str.contains('gemini')\", inplace=True\n",
    ")\n",
    "qwen_ai_perfs = compute_ai_perfs(human_labels_easy_set, qwen_ai_labels, questions)\n",
    "order = [f\"Qwen/Qwen2.5-VL-{n}B-Instruct\" for n in [3, 7, 7]] + [\"gemini-2.0-flash-001\"]\n",
    "plot_ai_perfs(qwen_ai_perfs, order, questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AI perform on the hard posts?\n",
    "The hard posts cannot be evaluated on single-labels ground truth like the easy posts.\n",
    "Instead we compare how often AI changes its mind, i.e. how self-consistent its answers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [f\"gemma-3-27b-it_{i:02d}\" for i in range(3)]\n",
    "ai_labels = load_ai_labels(folders, questions, comment_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_answers_hard = (\n",
    "    pd.melt(ai_labels, id_vars=\"post_id\", value_vars=questions)\n",
    "    .groupby([\"post_id\", \"variable\"], as_index=False)[\"value\"]\n",
    "    .agg([(\"n_answers\", \"count\"), (\"n_unique\", \"nunique\")])\n",
    "    .assign(ai_consistent=lambda df: df[\"n_unique\"] == 1)\n",
    ")\n",
    "\n",
    "ai_answers_hard = pd.merge(ai_answers_hard, post_difficulty_granular, on=[\"post_id\", \"variable\"])\n",
    "ai_answers_hard\n",
    "# ai_answers_hard = pd.merge(ai_answers_hard, post_difficulty, on=\"post_id\", how=\"left\")\n",
    "# ai_answers_hard.groupby([\"variable\", \"is_easy\"])[\"self_consistent\"].value_counts(\n",
    "#     normalize=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_answers_hard.groupby([\"variable\", \"human_consistent\"])[\"ai_consistent\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "* Detecting Hedonic entertainment and Intolerance significantly decrease the self-consistency of AI\n",
    "* Against my expectations, there is no significant effect of human consistency on AI consistency (hypothesis was human are inconsistent in difficutl posts, and so will be AI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform logistic regression to analyze how variable type and human consistency\n",
    "# affect AI consistency\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "\n",
    "# Convert categorical variable to dummy variables\n",
    "model_data = ai_answers_hard.assign(\n",
    "    variable=lambda df: pd.Categorical(df[\"variable\"]),\n",
    "    human_consistent=lambda df: df[\"human_consistent\"].astype(int),\n",
    "    ai_consistent=lambda df: df[\"ai_consistent\"].astype(int),\n",
    ")\n",
    "\n",
    "# Fit logistic regression model\n",
    "logit_model = smf.logit(\"ai_consistent ~ C(variable) + C(human_consistent)\", data=model_data)\n",
    "result = logit_model.fit()\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(result.summary())\n",
    "\n",
    "# Display odds ratios\n",
    "print(\"\\nOdds Ratios:\")\n",
    "print(np.exp(result.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When is AI inconsistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "row = ai_answers_hard.query(\"~ai_consistent\").iloc[idx]\n",
    "ai_labels.query(\"post_id == @row.post_id\")[[row[\"variable\"], f\"{row['variable']}_comment\"]].to_dict(orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
