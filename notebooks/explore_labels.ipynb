{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.utils import enable_info_logs\n",
    "\n",
    "enable_info_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "fpath = (\n",
    "    Path(os.environ[\"DSS_HOME\"])\n",
    "    / \"toxicainment/2025-02-07-saxony-labeled-data/human-labels.csv\"\n",
    ")\n",
    "human_labels = pd.read_csv(fpath, dtype={\"post_id\": str})\n",
    "human_labels = (\n",
    "    human_labels.sort_values(by=\"timestamp\")\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"is_saxony_election\": \"is_saxony\",\n",
    "            \"is_saxony_election_comment\": \"is_saxony_comment\",\n",
    "        }\n",
    "    )  # The column name was wrong while saving, but the UI correctly displayed that it does not HAVE to be politics in saxony\n",
    "    .groupby([\"post_id\", \"classification_by\"])\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")\n",
    "print(len(human_labels))\n",
    "# join num raters per post\n",
    "nraters_by_post = (\n",
    "    human_labels.groupby(\"post_id\", as_index=False)[\"classification_by\"]\n",
    "    .agg([\"nunique\"])\n",
    "    .rename(columns={\"nunique\": \"nraters\"})\n",
    ")\n",
    "human_labels = pd.merge(human_labels, nraters_by_post, on=\"post_id\", how=\"left\")\n",
    "\n",
    "questions = [\n",
    "    \"is_political\",\n",
    "    \"is_saxony\",\n",
    "    \"is_intolerant\",\n",
    "    \"is_hedonic_entertainment\",\n",
    "    \"is_eudaimonic_entertainment\",\n",
    "]\n",
    "comment_cols = [f\"{col}_comment\" for col in questions]\n",
    "human_labels[questions] = human_labels[questions].apply(\n",
    "    lambda s: s.map({\"yes\": 1, \"no\": 0, \"0\": 0, \"1\": 1})\n",
    ")\n",
    "\n",
    "human_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_balance = (\n",
    "    human_labels[questions].apply(lambda s: s.value_counts(normalize=True)).round(3)\n",
    ")\n",
    "label_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_scalars_for_questions\n",
    "\n",
    "plot_scalars_for_questions(label_balance.max(axis=0), questions, \"Majority Class %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage Agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement(human_labels: pd.DataFrame, question: str) -> float:\n",
    "    df = pd.crosstab(human_labels[\"post_id\"], human_labels[question])\n",
    "    full_agreement = (df[0] == 0) | (df[1] == 0)\n",
    "    return float(full_agreement.mean())\n",
    "\n",
    "\n",
    "agreements = [compute_agreement(human_labels, q) for q in questions]\n",
    "print(agreements)\n",
    "plot_scalars_for_questions(\n",
    "    agreements, questions, \"Percentage Agreement\", x_reversed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bench_lib.evaluation import krippendorf_alpha\n",
    "\n",
    "\n",
    "alphas = [\n",
    "    krippendorf_alpha(human_labels[\"post_id\"], human_labels[q]) for q in questions\n",
    "]\n",
    "fig = plot_scalars_for_questions(\n",
    "    alphas, questions, \"Krippendorff's alpha\", x_reversed=True\n",
    ")\n",
    "# fig.savefig(\"imgs/krippendorffs_alpha.pdf\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_constistent_df = pd.melt(\n",
    "    (human_labels.groupby(\"post_id\")[questions].nunique() == 1).reset_index(),\n",
    "    id_vars=\"post_id\",\n",
    "    value_vars=questions,\n",
    "    value_name=\"human_consistent\",\n",
    ")\n",
    "human_constistent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AI perform on the easy posts?\n",
    "Assuming the easy posts are those where humans are consistent.\n",
    "We assume here the ground truth to be the human labels, which are unique by construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels_long = pd.melt(human_labels, id_vars=\"post_id\", value_vars=questions)\n",
    "ground_truth_long = pd.merge(\n",
    "    human_labels_long,\n",
    "    human_constistent_df.query(\"human_consistent\"),\n",
    "    on=(\"post_id\", \"variable\"),\n",
    ").drop_duplicates()\n",
    "ground_truth_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import load_ai_labels, compute_ai_perfs\n",
    "\n",
    "\n",
    "gemma3_folders = [\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-27b-it_00\",\n",
    "    \"gemini-2.0-flash-001\",\n",
    "]\n",
    "gemma3_ai_labels_long = load_ai_labels(gemma3_folders, questions, comment_cols)\n",
    "gemma3_ai_perfs = compute_ai_perfs(ground_truth_long, gemma3_ai_labels_long, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sizes = ground_truth_long.groupby(\"variable\", as_index=False).size()\n",
    "plot_scalars_for_questions(gt_sizes[\"size\"], gt_sizes[\"variable\"], \"Ground truth size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma3_ai_perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_ai_perfs\n",
    "\n",
    "\n",
    "g3_order = [f\"google/gemma-3-{n}b-it\" for n in [4, 12, 27]] + [\n",
    "    \"google/gemini-2.0-flash-001\"\n",
    "]\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "for y in metrics:\n",
    "    fig = plot_ai_perfs(gemma3_ai_perfs, g3_order, list(reversed(questions)), y=y)\n",
    "    fig.savefig(f\"imgs/gemma3_ai_perfs_{y}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_ai_labels = load_ai_labels(\n",
    "    folders=[\"qwen-2.5-vl\", \"gemini-2.0-flash-001\"],\n",
    "    questions=questions,\n",
    "    comment_cols=comment_cols,\n",
    ")\n",
    "qwen_ai_labels.query(\n",
    "    \"`Model ID`.str.contains('Qwen') or `Model ID`.str.contains('gemini')\", inplace=True\n",
    ")\n",
    "qwen_ai_perfs = compute_ai_perfs(ground_truth_long, qwen_ai_labels, questions)\n",
    "order = [f\"Qwen/Qwen2.5-VL-{n}B-Instruct\" for n in [3, 7, 72]] + [\n",
    "    \"google/gemini-2.0-flash-001\"\n",
    "]\n",
    "for y in metrics:\n",
    "    fig = plot_ai_perfs(qwen_ai_perfs, order, x_order=list(reversed(questions)), y=y)\n",
    "    fig.savefig(f\"imgs/qwen_ai_perfs_{y}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AI perform on the hard posts?\n",
    "The hard posts cannot be evaluated on single-labels ground truth like the easy posts.\n",
    "Instead we compare how often AI changes its mind, i.e. how self-consistent its answers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [f\"gemma-3-27b-it_{i:02d}\" for i in range(3)]\n",
    "ai_labels = load_ai_labels(folders, questions, comment_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_consistency_df = (\n",
    "    ai_labels.groupby([\"post_id\", \"variable\"], as_index=False)[\"value\"]\n",
    "    .agg([(\"n_answers\", \"count\"), (\"n_unique\", \"nunique\")])\n",
    "    .assign(ai_consistent=lambda df: df[\"n_unique\"] == 1)\n",
    ")\n",
    "\n",
    "joint_consistency_df = pd.merge(\n",
    "    ai_consistency_df, human_constistent_df, on=[\"post_id\", \"variable\"]\n",
    ")\n",
    "joint_consistency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consistency_by_question = (\n",
    "    pd.melt(\n",
    "        joint_consistency_df.rename(columns={\"variable\": \"question\"}),\n",
    "        id_vars=[\"post_id\", \"question\"],\n",
    "        value_vars=[\"ai_consistent\", \"human_consistent\"],\n",
    "    )\n",
    "    .groupby([\"question\", \"variable\"], as_index=False)[\"value\"]\n",
    "    .mean()\n",
    ")\n",
    "consistency_by_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_labels.groupby(\"variable\", as_index=False)[\"value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels_long.groupby(\"variable\", as_index=False)[\"value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "sns.barplot(\n",
    "    data=consistency_by_question,\n",
    "    x=\"question\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"Consistency\")\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha=\"right\")\n",
    "ax.legend(\n",
    "    title=\"Annotator\",\n",
    "    bbox_to_anchor=(1.0, 1),\n",
    "    loc=\"upper left\",\n",
    ")\n",
    "ax.grid(alpha=0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "* Detecting Hedonic entertainment and Intolerance significantly decrease the self-consistency of AI\n",
    "* Against my expectations, there is no significant effect of human consistency on AI consistency (hypothesis was human are inconsistent in difficutl posts, and so will be AI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform logistic regression to analyze how variable type and human consistency\n",
    "# affect AI consistency\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "\n",
    "# Convert categorical variable to dummy variables\n",
    "model_data = joint_consistency_df.assign(\n",
    "    variable=lambda df: pd.Categorical(df[\"variable\"]),\n",
    "    human_consistent=lambda df: df[\"human_consistent\"].astype(int),\n",
    "    ai_consistent=lambda df: df[\"ai_consistent\"].astype(int),\n",
    ")\n",
    "\n",
    "# Fit logistic regression model\n",
    "logit_model = smf.logit(\n",
    "    \"ai_consistent ~ C(variable) + C(human_consistent)\", data=model_data\n",
    ")\n",
    "result = logit_model.fit()\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(result.summary())\n",
    "\n",
    "# Display odds ratios\n",
    "print(\"\\nOdds Ratios:\")\n",
    "print(np.exp(result.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When is AI inconsistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "row = ai_answers_hard.query(\"~ai_consistent\").iloc[idx]\n",
    "ai_labels.query(\"post_id == @row.post_id\")[\n",
    "    [row[\"variable\"], f\"{row['variable']}_comment\"]\n",
    "].to_dict(orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
