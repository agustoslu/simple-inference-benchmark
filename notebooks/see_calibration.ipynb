{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72d744",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d4e46",
   "metadata": {},
   "source": [
    "get krippendorff alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bench_lib.evaluation import load_ai_labels, load_human_labels, krippendorf_alpha\n",
    "from scipy.stats import entropy\n",
    "from scipy.special import rel_entr\n",
    "from functools import reduce\n",
    "\n",
    "human_labels, questions, comment_cols = load_human_labels()\n",
    "alphas = [\n",
    "    krippendorf_alpha(human_labels[\"post_id\"], human_labels[q]) for q in questions\n",
    "]\n",
    "alphas  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb204e0b",
   "metadata": {},
   "source": [
    "get disagreement subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03aa377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_disagreement(x):\n",
    "    for q in questions:\n",
    "        values = x[q].dropna().unique()\n",
    "        if len(values) > 1:\n",
    "            try:\n",
    "                if krippendorf_alpha(x[\"post_id\"], x[q]) < 0.60: # krippendorf's alpha lower bound for tentative results\n",
    "                    return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return False\n",
    "\n",
    "disagreement_subset = human_labels[\n",
    "    [\"post_id\", \"classification_by\"] + questions\n",
    "].groupby(\"post_id\").filter(has_disagreement)\n",
    "disagreement_subset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e093f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreement_subset_long = pd.melt(\n",
    "    disagreement_subset, id_vars=[\"post_id\", \"classification_by\"], value_vars=questions)\n",
    "disagreement_subset_long.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca6474",
   "metadata": {},
   "source": [
    "calculate entropy per group (question, post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f170a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(labels):\n",
    "    value_counts = labels.value_counts(normalize=True) # get probs\n",
    "    return entropy(value_counts, base=2) # base 2 for bits\n",
    "\n",
    "entropies = []\n",
    "for q in questions:\n",
    "    entropies_q = (\n",
    "        disagreement_subset.groupby(\"post_id\")[q]\n",
    "        .apply(lambda x: compute_entropy(x.dropna()))\n",
    "        .reset_index(name=f\"{q}_entropy\")\n",
    "    )\n",
    "    entropies.append(entropies_q)\n",
    "\n",
    "entropy_df = reduce(lambda left, right: left.merge(right, on=\"post_id\"), entropies)\n",
    "entropy_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467c950c",
   "metadata": {},
   "source": [
    "Stratify (Low, Modedarate, High Entropies)\n",
    "Due to number of annotators participated the variance is low. array([0.91829583, 1. , 0.81127812, 0. ]). Still we map them 1 to high entropy, 0 to low entropy and \n",
    "anything between 0 and 1 to moderate entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_entropy(entropy):\n",
    "    if entropy == 0:\n",
    "        return \"Low\"\n",
    "    elif entropy == 1:\n",
    "        return \"High\"\n",
    "    else:\n",
    "        return \"Moderate\"\n",
    "\n",
    "entropy_cols = [col for col in entropy_df.columns if col.endswith(\"_entropy\")]\n",
    "\n",
    "for col in entropy_cols:\n",
    "    bin_col = col.replace(\"_entropy\", \"_entropy_bin\")\n",
    "    entropy_df[bin_col] = entropy_df[col].apply(stratify_entropy)\n",
    "\n",
    "entropy_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c6aca",
   "metadata": {},
   "source": [
    "get ai labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0533b911",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-VL-72B-Instruct\"\n",
    "]\n",
    "\n",
    "run_folders = [\n",
    "    \"toxicainment_videos_log_Temp_0_7_1\",\n",
    "    \"toxicainment_videos_log_Temp_0_7_2\",\n",
    "    \"toxicainment_videos_log_Temp_0_7_3\",\n",
    "    \"toxicainment_videos_log_Temp_0_7_4\",\n",
    "    \"toxicainment_videos_log_Temp_0_7_5\"\n",
    "]\n",
    "\n",
    "base_path = \"/home/tanalp/toxicainment/simple_inference_benchmark_results\"\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model in models:\n",
    "    folders = [\n",
    "        f\"{base_path}/{run_folder}/{model}\"\n",
    "        for run_folder in run_folders\n",
    "    ]\n",
    "    ai_labels_long = pd.concat(\n",
    "        [load_ai_labels([folder], questions, comment_cols) for folder in folders],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    model_results[model] = ai_labels_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4da71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels_long, _, _ = load_human_labels(long=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_soft = (\n",
    "    human_labels_long\n",
    "    .groupby(['post_id', 'variable'])['value']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'value': 'human_soft_label'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759eba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# get mean of 5 stochastic runs for each model\n",
    "for model, ai_labels_long in model_results.items():\n",
    "    model_soft = (\n",
    "        ai_labels_long\n",
    "        .groupby(['post_id', 'variable'])['value']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'value': 'model_soft_label'})\n",
    "    )\n",
    "    merged = pd.merge(human_soft, model_soft, on=['post_id', 'variable'])\n",
    "    merged['brier_score'] = (merged['human_soft_label'] - merged['model_soft_label']) ** 2 # more like a cost function, mean squared error https://en.wikipedia.org/wiki/Brier_score\n",
    "    merged['model'] = model\n",
    "    results.append(merged)\n",
    "\n",
    "all_results = pd.concat(results, ignore_index=True)\n",
    "all_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555143db",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for q in questions:\n",
    "    for _, row in entropy_df.iterrows():\n",
    "        rows.append({\n",
    "            \"post_id\": row[\"post_id\"],\n",
    "            \"question\": q,\n",
    "            \"entropy\": row[f\"{q}_entropy\"],\n",
    "            \"entropy_bin\": row[f\"{q}_entropy_bin\"]\n",
    "        })\n",
    "\n",
    "\n",
    "new_entropy = pd.DataFrame(rows)\n",
    "new_entropy = new_entropy.sort_values([\"entropy_bin\", \"question\", \"post_id\"])\n",
    "new_entropy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e19503",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = all_results.merge(\n",
    "    new_entropy[[\"post_id\", \"question\", \"entropy_bin\"]],\n",
    "    left_on=[\"post_id\", \"variable\"],\n",
    "    right_on=[\"post_id\", \"question\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "summary = (\n",
    "    merged.groupby([\"model\", \"variable\", \"entropy_bin\"])[\"brier_score\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values([\"model\", \"variable\", \"entropy_bin\"])\n",
    ")\n",
    "\n",
    "summary.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f2c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_brier_scores(summary):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        data=summary,\n",
    "        x=\"entropy_bin\",\n",
    "        y=\"brier_score\",\n",
    "        hue=\"model\",\n",
    "        ci=None,\n",
    "        palette=\"rocket\",\n",
    "        order=[\"Low\", \"Moderate\", \"High\"]\n",
    "    )\n",
    "    plt.title(\"Brier Scores by Model and Entropy Bin\")\n",
    "    plt.xlabel(\"Entropy Bin\")\n",
    "    plt.ylabel(\"Brier Score\")\n",
    "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_brier_scores(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe496c06",
   "metadata": {},
   "source": [
    "get brier plot with variables instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    all_results.groupby([\"model\", \"variable\"])[\"brier_score\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .sort_values([\"model\", \"variable\"])\n",
    ")\n",
    "\n",
    "summary.head(3)\n",
    "\n",
    "\n",
    "def plot_brier_scores(summary):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(\n",
    "        data=summary,\n",
    "        x=\"variable\",\n",
    "        y=\"brier_score\",\n",
    "        hue=\"model\",\n",
    "        ci=None,\n",
    "        palette=\"rocket\"\n",
    "    )\n",
    "    plt.title(\"Brier Scores by Model and Variable\")\n",
    "    plt.xlabel(\"Variable\")\n",
    "    plt.ylabel(\"Brier Score\")\n",
    "    plt.legend(title=\"Model\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_brier_scores(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
