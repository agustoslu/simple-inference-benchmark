{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.utils import enable_info_logs\n",
    "\n",
    "enable_info_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import load_human_labels\n",
    "\n",
    "\n",
    "human_labels, questions, comment_cols = load_human_labels()\n",
    "human_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_balance = (\n",
    "    human_labels[questions].apply(lambda s: s.value_counts(normalize=True)).round(3)\n",
    ")\n",
    "label_balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_scalars_for_questions\n",
    "\n",
    "plot_scalars_for_questions(\n",
    "    label_balance.max(axis=0), questions, \"Majority Class %\", x_reversed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage Agreement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels_long = pd.melt(\n",
    "    human_labels, id_vars=[\"classification_by\", \"post_id\"], value_vars=questions\n",
    ")\n",
    "human_labels_long.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import compute_agreement_score\n",
    "\n",
    "human_agreement = compute_agreement_score(human_labels_long)\n",
    "human_agreement.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agreement_by_question = human_agreement.groupby(\"variable\", as_index=False).agg(\n",
    "    avg_agreement_score=(\"agreement_score\", \"mean\"),\n",
    "    avg_full_agreement=(\"full_agreement\", \"mean\"),\n",
    ")\n",
    "agreement_by_question.sort_values(\"avg_full_agreement\", inplace=True)\n",
    "agreement_by_question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scalars_for_questions(\n",
    "    agreement_by_question[\"avg_full_agreement\"],\n",
    "    agreement_by_question[\"variable\"],\n",
    "    \"Posts with Full Agreement [%]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bench_lib.evaluation import krippendorf_alpha\n",
    "\n",
    "\n",
    "alphas = [\n",
    "    krippendorf_alpha(human_labels[\"post_id\"], human_labels[q]) for q in questions\n",
    "]\n",
    "fig = plot_scalars_for_questions(\n",
    "    alphas, questions, \"Krippendorff's alpha\", x_reversed=True\n",
    ")\n",
    "# fig.savefig(\"imgs/krippendorffs_alpha.pdf\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AI perform on the easy posts?\n",
    "Assuming the easy posts are those where humans are consistent.\n",
    "We assume here the ground truth to be the human labels, which are unique by construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_long = pd.merge(\n",
    "    human_labels_long,\n",
    "    human_agreement.query(\"full_agreement\"),\n",
    "    on=(\"post_id\", \"variable\"),\n",
    ").drop_duplicates()\n",
    "ground_truth_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import load_ai_labels, compute_ai_perfs\n",
    "\n",
    "\n",
    "gemma3_folders = [\n",
    "    \"gemma-3-4b-it\",\n",
    "    \"gemma-3-12b-it\",\n",
    "    \"gemma-3-27b-it_00\",\n",
    "    \"gemini-2.5-pro-noschema\",\n",
    "]\n",
    "gemma3_ai_labels_long = load_ai_labels(gemma3_folders, questions, comment_cols)\n",
    "gemma3_ai_perfs = compute_ai_perfs(ground_truth_long, gemma3_ai_labels_long, questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_sizes = ground_truth_long.groupby(\"variable\", as_index=False).size()\n",
    "plot_scalars_for_questions(gt_sizes[\"size\"], gt_sizes[\"variable\"], \"Ground truth size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import plot_ai_perfs\n",
    "\n",
    "\n",
    "g3_order = [f\"google/gemma-3-{n}b-it\" for n in [4, 12, 27]] + [\n",
    "    \"google/gemini-2.5-pro-preview-03-25\"\n",
    "]\n",
    "metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "for y in metrics:\n",
    "    fig = plot_ai_perfs(gemma3_ai_perfs, g3_order, list(reversed(questions)), y=y)\n",
    "    fig.savefig(f\"imgs/gemma3_ai_perfs_{y}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_ai_labels = load_ai_labels(\n",
    "    folders=[\"qwen-2.5-vl\", \"gemini-2.5-pro-noschema\"],\n",
    "    questions=questions,\n",
    "    comment_cols=comment_cols,\n",
    ")\n",
    "qwen_ai_labels.query(\n",
    "    \"`Model ID`.str.contains('Qwen') or `Model ID`.str.contains('gemini')\", inplace=True\n",
    ")\n",
    "qwen_ai_perfs = compute_ai_perfs(ground_truth_long, qwen_ai_labels, questions)\n",
    "order = [f\"Qwen/Qwen2.5-VL-{n}B-Instruct\" for n in [3, 7, 72]] + [\n",
    "    \"google/gemini-2.5-pro-preview-03-25\"\n",
    "]\n",
    "for y in metrics:\n",
    "    fig = plot_ai_perfs(qwen_ai_perfs, order, x_order=list(reversed(questions)), y=y)\n",
    "    fig.savefig(f\"imgs/qwen_ai_perfs_{y}.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does AI perform on the hard posts?\n",
    "The hard posts cannot be evaluated on single-labels ground truth like the easy posts.\n",
    "Instead we compare how often AI changes its mind, i.e. how self-consistent its answers are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"self-consistency\"]\n",
    "ai_labels = load_ai_labels(folders, questions, comment_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_consistency_df = compute_agreement_score(\n",
    "    ai_labels, groupby=[\"Model ID\", \"post_id\", \"variable\"]\n",
    ")\n",
    "\n",
    "joint_consistency_df = pd.merge(\n",
    "    ai_consistency_df,\n",
    "    human_agreement,\n",
    "    on=[\"post_id\", \"variable\"],\n",
    "    suffixes=(\"_ai\", \"_human\"),\n",
    ")\n",
    "joint_consistency_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "* Detecting Hedonic entertainment and Intolerance significantly decrease the self-consistency of AI\n",
    "* Against my expectations, there is no significant effect of human consistency on AI consistency (hypothesis was human are inconsistent in difficutl posts, and so will be AI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_consistency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert categorical variable to dummy variables\n",
    "model_data = joint_consistency_df.assign(\n",
    "    model_id=lambda df: df[\"Model ID\"],\n",
    "    model_size=lambda df: df[\"Model ID\"].str.lower().str.extract(r'-(\\d+)b-').astype(int),\n",
    "    model_family=lambda df: np.where(df[\"Model ID\"].str.contains(\"Qwen\"), \"Qwen2.5\", \"Gemma3\"),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression to analyze how variable type and human consistency\n",
    "# affect AI consistency\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "ols_model = smf.ols(\n",
    "    \"agreement_score_ai ~ model_size + C(model_family) + C(variable) + agreement_score_human\",\n",
    "    data=model_data,    \n",
    ")\n",
    "result = ols_model.fit()\n",
    "print(\"Linear Regression Results:\")\n",
    "print(result.summary())\n",
    "\n",
    "# Display coefficients\n",
    "print(\"\\nCoefficients:\")\n",
    "print(result.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "fig = sns.lmplot(\n",
    "    joint_consistency_df,\n",
    "    x=\"agreement_score_human\",\n",
    "    y=\"agreement_score_ai\",\n",
    "    hue=\"variable\",\n",
    "    row=\"Model ID\",\n",
    "    col=\"variable\",\n",
    "    y_jitter=0.02,\n",
    "    x_jitter=0.02,\n",
    ")\n",
    "fig.savefig(\"imgs/ai_consistency_by_human_consistency.pdf\", bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bench_lib.evaluation import bertscore_alignment, plot_alignment_table\n",
    "\n",
    "df = pd.read_csv(\"model_labels.csv\")\n",
    "bertscore_df = bertscore_alignment(df)\n",
    "fig = plot_alignment_table(bertscore_df)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
